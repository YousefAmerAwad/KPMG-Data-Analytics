{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Project using Jupyter Notebook\n",
    "\n",
    "## Introduction\n",
    "In this data analysis project, we will be working with datasets from KPMG_VI_New_raw_data_update_final.xlsx file. The data consists of three datasets: transaction, CustomerDemographic, and CustomerAddress.\n",
    "\n",
    "## Goals\n",
    "The primary goals of this analysis are to explore the datasets, clean the data, perform descriptive statistics, and gain insights from the data.\n",
    "\n",
    "## Datasets\n",
    "1. `transaction`: Contains information about various transactions.\n",
    "2. `CustomerDemographic`: Contains demographic information of customers.\n",
    "3. `CustomerAddress`: Contains address details of customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "##  1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. Read the Excel file and display datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "\n",
    "# Excel file containing the datasets\n",
    "file = 'KPMG_VI_New_raw_data_update_final.xlsx'\n",
    "\n",
    "# Load the datasets from different sheets in the Excel file\n",
    "# Sheet 1: transaction dataset\n",
    "transaction = pd.read_excel(file, sheet_name=1)\n",
    "\n",
    "# Sheet 2: NewCustomerList dataset\n",
    "NewCustomerList = pd.read_excel(file, sheet_name=2)\n",
    "\n",
    "# Sheet 3: CustomerDemographic dataset\n",
    "CustomerDemographic = pd.read_excel(file, sheet_name=3)\n",
    "\n",
    "# Sheet 4: CustomerAddress dataset\n",
    "CustomerAddress = pd.read_excel(file, sheet_name=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Transaction datasets\n",
    "\n",
    "transaction.columns = [list(transaction.iloc[0])]\n",
    "transaction.drop (index = 0, inplace= True)\n",
    "transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NewCustomerList datasets\n",
    "\n",
    "NewCustomerList.columns = [list(NewCustomerList.iloc[0])]\n",
    "NewCustomerList.drop (index = 0, inplace= True)\n",
    "NewCustomerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CustomerDemographic datasets\n",
    "\n",
    "CustomerDemographic.columns = [list(CustomerDemographic.iloc[0])]\n",
    "CustomerDemographic.drop (index = 0, inplace= True)\n",
    "CustomerDemographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CustomerAddress datasets\n",
    "\n",
    "CustomerAddress.columns = [list(CustomerAddress.iloc[0])]\n",
    "CustomerAddress.drop (index = 0, inplace= True)\n",
    "CustomerAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp; 3.1 CustomerDemographic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "# The info() method provides a summary of the dataset, including the data types and non-null counts of each column.\n",
    "CustomerDemographic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  Comments: </b> </h3>  \n",
    " \n",
    "* > `The CustomerDemographic dataset contains null values in some columns, such as 'last_name', 'DOB', 'job_title', etc.` <br> `However, we do not need to delete these null values since they are not crucial for the analysis.`<br> `Moreover, deleting them could lead to data loss and affect foreign key constraints.`<br> <br> \n",
    "\n",
    "* > `We have decided to handle the missing values in the 'tenure' column by replacing them with the mean value of the non-missing 'tenure' values. This approach allows us to impute reasonable values for the missing entries, maintaining the integer data type for consistency in the 'tenure' column.`\n",
    "\n",
    "\n",
    "* > `The data types of some columns in the CustomerDemographic dataset need to be changed to more appropriate types. We will use the convert_dtypes() method for this purpose.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types of columns using `convert_dtypes()`\n",
    "\n",
    "# This method automatically infers and converts data types for better representation and analysis.\n",
    "CustomerDemographic = CustomerDemographic.convert_dtypes()\n",
    "CustomerDemographic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the missing values in the 'tenure' column by replacing them with the mean value of the non-missing 'tenure' values\n",
    "\n",
    "# Calculate the mean of the 'tenure' column excluding missing values\n",
    "mean_tenure = int(CustomerDemographic['tenure'].mean())\n",
    "\n",
    "# Fill the missing values with the mean value and display all unique values\n",
    "CustomerDemographic['tenure'] = CustomerDemographic['tenure'].fillna(mean_tenure)\n",
    "np.unique(CustomerDemographic['tenure'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the dataset\n",
    "CustomerDemographic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b> Comments: </b> </h3>  \n",
    " \n",
    "* > `The 'default' column contains garbage data and is not relevant for our analysis.` <br> `Therefore, we will remove this column to ensure data cleanliness and improve the quality of our analysis.`<br> <br> \n",
    "* > ` The 'gender' column contains garbage data such as 'F', 'U', etc., which seems inconsistent with standard gender categories. To address this issue, we will inspect all unique gender values that are not either male or female. We will then update these values based on customer names or other available information to ensure accurate gender representation.` \n",
    "* > `After checking all unique values in each column of the 'CustomerDemographic' dataset, we will thoroughly examine the data for any additional inconsistencies or erroneous entries.` <br> ` We aim to maintain data integrity and enhance the reliability of our analysis by identifying and rectifying any discrepancies.`<br> <br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the 'default' column\n",
    "CustomerDemographic.drop(columns='default', inplace=True)\n",
    "\n",
    "# Display the updated dataset after removing the 'default' column\n",
    "CustomerDemographic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values of selected columns to identify potential garbage values or inconsistencies\n",
    "print('gender',np.unique(CustomerDemographic['gender'].values))\n",
    "print('wealth_segment',np.unique(CustomerDemographic['wealth_segment'].values))\n",
    "print('deceased_indicator',np.unique(CustomerDemographic['deceased_indicator'].values))\n",
    "print('owns_car',np.unique(CustomerDemographic['owns_car'].values))\n",
    "print('tenure',np.unique(CustomerDemographic['tenure'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b> Comments: </b> </h3>  \n",
    " \n",
    "* > `For the 'gender' column, we will keep 'Female', 'Male', and 'U' (Unknown) as valid gender values and update the rest of the values to align with these categories. For example, we will change 'M' to 'Male', 'F' to 'Female', and 'Femal' to 'Female' to ensure consistency in gender representation.`<br> <br> \n",
    "* > `In the 'owns_car' column, we will modify the values to be consistent with the 'deceased_indicator' column, where 'Y' represents 'Yes', and 'N' represents 'No'. This standardization ensures uniformity and clarity in the representation of car ownership in our dataset.`<br> <br> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will standardize the values in the 'gender' column to ensure consistency and clarity in gender representation.\n",
    "\n",
    "# Mapping for 'gender' column values\n",
    "gender_mapping = {\n",
    "    'M': 'Male',\n",
    "    'F': 'Female',\n",
    "    'Femal': 'Female'\n",
    "}\n",
    "CustomerDemographic['gender'] = CustomerDemographic['gender'].replace(gender_mapping)\n",
    "\n",
    "# Display the updated unique values of the 'gender' column after cleaning\n",
    "np.unique(CustomerDemographic['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will modify the values in the 'owns_car' column to ensure consistency with the 'deceased_indicator' column.\n",
    "\n",
    "# Mapping for 'owns_car' column values\n",
    "owns_car_mapping = {\n",
    "    'Yes': 'Y',\n",
    "    'No': 'N'\n",
    "}\n",
    "\n",
    "CustomerDemographic['owns_car'] = CustomerDemographic['owns_car'].replace(owns_car_mapping)\n",
    "\n",
    "# Display the updated unique values of the 'owns_car' column after cleaning\n",
    "np.unique(CustomerDemographic['owns_car'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### &emsp; 3.2 CustomerAddress Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "# The info() method provides a summary of the dataset, including the data types and non-null counts of each column.\n",
    "CustomerAddress.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  Comments: </b> </h3>  \n",
    " \n",
    "* > `The CustomerAddress dataset contains 3999 entries across 6 columns. `<br> `Fortunately, there are no null values in any of the columns, indicating complete data availability.`<br> <br> \n",
    "\n",
    "* > `The data type of all columns in the CustomerAddress dataset is currently set as 'object', which represents generic text strings. To ensure more appropriate data representation and enable efficient analysis, we will proceed with data type conversions.`<br> <br> \n",
    "\n",
    "* > `We observe that there is a slight inconsistency in the number of customer IDs between the CustomerAddress dataset and the CustomerDemographic dataset. `<br>`While CustomerAddress contains 3999 unique customer IDs, the CustomerDemographic dataset has 4000 unique customer IDs.`<br>` This indicates that one customer's address information might be missing or not available in the CustomerAddress dataset.`<br> <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types of columns using `convert_dtypes()`\n",
    "\n",
    "# This method automatically infers and converts data types for better representation and analysis.\n",
    "CustomerAddress = CustomerAddress.convert_dtypes()\n",
    "CustomerAddress.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the missing customer IDs in the 'CustomerAddress' dataset, we first generate a list of all IDs from 1 to 4000.\n",
    "# Then, we extract the existing customer IDs from the 'customer_id' column of the 'CustomerAddress' dataset.\n",
    "# As the 'customer_id' column may have a nested structure, we flatten it using np.ravel() and convert it to a list.\n",
    "# Next, we compare the list of all IDs to the list of existing IDs to identify the missing IDs.\n",
    "\n",
    "# Find missing IDs\n",
    "all_ids = list(range(1, 4001))  # Generate a list of all IDs from 1 to 4000\n",
    "existing_ids = CustomerAddress['customer_id'].values  # Extract existing customer IDs\n",
    "existing_ids = np.ravel(existing_ids).tolist()  # Flatten and convert to a list\n",
    "missing_ids = [x for x in all_ids if x not in existing_ids]  # Identify missing IDs\n",
    "\n",
    "# Display the list of missing customer IDs\n",
    "missing_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  Comments: </b> </h3>  \n",
    " \n",
    "* > `After identifying the missing IDs, we found four missing customer IDs: [3, 10, 22, 23].`<br> <br> \n",
    "\n",
    "* > `However, the 'CustomerAddress' dataset contains 3999 unique customer IDs, which suggests that there might be three duplicated IDs or incorrect entries in the dataset.`<br> <br> \n",
    "\n",
    "* > `To further investigate, we will check for duplicated customer IDs in the 'CustomerAddress' dataset and obtain the count of occurrences for each duplicated ID. `<br>`If there are no duplicated customer IDs in the dataset, we proceed to check for wrong IDs.`<br>` Wrong IDs are customer IDs that are not between 1 and 4000, suggesting data entry errors or incorrect entries. We display any wrong IDs found.`<br> <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated ids\n",
    "\n",
    "#boolean Series of duplicated_ids\n",
    "is_duplicated_ids = CustomerAddress['customer_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated customer IDs\n",
    "duplicated_ids = CustomerAddress['customer_id'][is_duplicated_ids]\n",
    "\n",
    "# Display the duplicated customer IDs\n",
    "duplicated_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  Comments: </b> </h3>  \n",
    " \n",
    "* > `Since no duplicated customer IDs were found in the 'CustomerAddress' dataset, we can conclude that any missing IDs are`<br> <br> \n",
    "\n",
    "* > `likely due to wrong IDs (IDs not between 1 and 4000). It's essential to investigate and handle these wrong IDs to ensure data accuracy and reliability in our analysis`<br> <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find those wrong ids\n",
    "\n",
    "#list of wrong ids\n",
    "wrong_ids = [x for x in existing_ids if x not in all_ids]\n",
    "\n",
    "#Display the list of wrong customer IDs\n",
    "wrong_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  Conclusion: </b> </h3>  \n",
    " \n",
    "* > `The code output shows three wrong customer IDs: [4001, 4002, 4003]. Although these IDs fall outside the valid range of customer IDs (1 to 4000), the information associated with these IDs appears to be correct. It suggests that these customers'  data exists, but they were assigned incorrect IDs, possibly due to data entry errors or incorrect records. Unfortunately, we do not have information about the correct IDs for these records, and we cannot delete them from the dataset as they are valid entries.`<br> <br> \n",
    "\n",
    "* > `Our primary concern is to address this data quality issue. Since we cannot correct the wrong customer IDs without knowing the accurate customer IDs or their corresponding records, it is crucial to establish communication with the data provider.`<br> <br> \n",
    "\n",
    "* > `Additionally, there is one customer ID missing from the 'CustomerAddress' dataset. Unfortunately, we do not have specific information about this missing ID, and it could be any valid ID that we have not captured.`<br> <br> \n",
    "\n",
    "* > `To ensure data accuracy and integrity, it is essential to review and validate the entire 'CustomerAddress' dataset and work together with the data provider to acquire the correct dataset. By obtaining the right data, we can ensure reliable analysis and make informed decisions based on complete and trustworthy information.`<br> <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### &emsp; 3.3 Transaction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "# The info() method provides a summary of the dataset, including the data types and non-null counts of each column.\n",
    "transaction.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  comments: </b> </h3>  \n",
    "\n",
    "* > `The data type of all columns in the transaction dataset is currently set as 'object', which represents generic text strings. To ensure more appropriate data representation and enable efficient analysis, we will proceed with data type conversions.`<br> <br> \n",
    "\n",
    "* > `We need to remove all null values to ensure data integrity and maintain the reliability of our analysis`<br> <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\n",
    "transaction.dropna(inplace=True)\n",
    "\n",
    "# Reset the index \n",
    "transaction.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Modify the 'transaction_id' column to have consecutive values\n",
    "transaction['transaction_id'] = range(1, len(transaction) + 1)\n",
    "transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types of columns using `convert_dtypes()`\n",
    "\n",
    "# This method automatically infers and converts data types for better representation and analysis.\n",
    "transaction = transaction.convert_dtypes()\n",
    "transaction.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### &emsp; 3.4 NewCustomerList Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewCustomerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "# The info() method provides a summary of the dataset, including the data types and non-null counts of each column.\n",
    "NewCustomerList.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  comments: </b> </h3>  \n",
    "\n",
    "* > `The dataset contains null values in some columns.However, we have chosen not to remove these null values. Removing them could potentially result in data loss, and it's crucial to maintain the data's integrity`<br> <br> \n",
    "\n",
    "* > ` The data types of some columns need to be converted to appropriate types to enhance the dataset's representation and analysis accuracy. We will use the necessary methods to perform these data type conversions.`<br> <br> \n",
    "\n",
    "* > ` There are three unnamed columns in the dataset. To ensure a complete understanding of the dataset's structure and meaning, we recommend reaching out to the data provider. Obtaining information about the purpose and relevance of these unnamed columns will aid in more accurate data analysis and interpretation.`<br> <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types of columns using `convert_dtypes()`\n",
    "\n",
    "# This method automatically infers and converts data types for better representation and analysis.\n",
    "NewNewCustomerList =NewCustomerList.convert_dtypes()\n",
    "NewNewCustomerList.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values of selected columns to identify potential garbage values or inconsistencies\n",
    "print('gender', np.unique(NewCustomerList['gender'].values))\n",
    "print('wealth_segment', np.unique(NewCustomerList['wealth_segment'].values))\n",
    "print('deceased_indicator', np.unique(NewCustomerList['deceased_indicator'].values))\n",
    "print('owns_car', np.unique(NewCustomerList['owns_car'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h3> <b>  comments: </b> </h3>  \n",
    "\n",
    "* > `All the displayed columns, contain clean and valid values without any unexpected entries or inconsistencies. This indicates that the dataset has been well-prepared, and the data is suitable for analysis without the need for further cleaning.`<br> <br> \n",
    "\n",
    "\n",
    "\n",
    "* > `In the 'owns_car' column, we will modify the values to be consistent with the 'deceased_indicator' column, where 'Y' represents 'Yes', and 'N' represents 'No'. This standardization ensures uniformity and clarity in the representation of car ownership in our dataset.`<br> <br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 4. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
